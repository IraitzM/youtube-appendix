{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014fd980-5d50-434e-bb50-52d2e71417f8",
   "metadata": {},
   "source": [
    "Unfortunately, you cannot use Modal in a notebook in the same way that you might from the command line. In short, this breaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0119a757-1110-4890-bffe-4dc4e06bd583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96adc63976d64b6b8c1478fd86e8b38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“</span> Initialized. <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">View run at </span><span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2; text-decoration: underline\">https://modal.com/apps/koaning/main/ap-xzAIthyNgV5kuFOkxl5KzO</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“\u001b[0m Initialized. \u001b[38;5;249mView run at \u001b[0m\u001b[4;38;5;249mhttps://modal.com/apps/koaning/main/ap-xzAIthyNgV5kuFOkxl5KzO\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb105bed0e042b088fa65de92dfc3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“</span> Created objects.\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â””â”€â”€ </span>ðŸ”¨ Created function func.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“\u001b[0m Created objects.\n",
       "\u001b[38;5;244mâ””â”€â”€ \u001b[0mðŸ”¨ Created function func.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c02b5005d234503bf5863c5da9d95e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">single call\n",
       "</pre>\n"
      ],
      "text/plain": [
       "single call\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=1, b=2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=1, b=2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">sequential calls\n",
       "</pre>\n"
      ],
      "text/plain": [
       "sequential calls\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=0, b=1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=0, b=1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=1, b=2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=1, b=2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=2, b=3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=2, b=3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=3, b=4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=3, b=4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=4, b=5\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=4, b=5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=5, b=6\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=5, b=6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=6, b=7\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=6, b=7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=7, b=8\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=7, b=8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=8, b=9\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=8, b=9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=9, b=10\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=9, b=10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=10, b=11\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=10, b=11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=11, b=12\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=11, b=12\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=12, b=13\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=12, b=13\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=13, b=14\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=13, b=14\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=14, b=15\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=14, b=15\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=15, b=16\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=15, b=16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=16, b=17\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=16, b=17\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=17, b=18\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=17, b=18\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=18, b=19\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=18, b=19\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a=19, b=20\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a=19, b=20\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">parallel call\n",
       "</pre>\n"
      ],
      "text/plain": [
       "parallel call\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Stopping app - uncaught exception raised locally: InvalidError(\"You can't run Function.map() or Function.for_each() from an async function. Use Function.map.aio()/Function.for_each.aio() instead.\").\n",
       "</span></pre>\n"
      ],
      "text/plain": [
       "\u001b[33mStopping app - uncaught exception raised locally: InvalidError(\"You can't run Function.map() or Function.for_each() from an async function. Use Function.map.aio()/Function.for_each.aio() instead.\").\n",
       "\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidError",
     "evalue": "You can't run Function.map() or Function.for_each() from an async function. Use Function.map.aio()/Function.for_each.aio() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNestedEventLoops\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/Development/probabl/.venv/lib/python3.12/site-packages/modal/_utils/async_utils.py:383\u001b[0m, in \u001b[0;36mAsyncOrSyncIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Runner() \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m run_async_gen(runner, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_iterable)\n",
      "File \u001b[0;32m~/Development/probabl/.venv/lib/python3.12/site-packages/synchronicity/async_utils.py:26\u001b[0m, in \u001b[0;36mRunner.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NestedEventLoops()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n",
      "\u001b[0;31mNestedEventLoops\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39mstarmap([(i, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m)])\n\u001b[0;32m---> 22\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/probabl/.venv/lib/python3.12/site-packages/modal/_utils/async_utils.py:386\u001b[0m, in \u001b[0;36mAsyncOrSyncIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m run_async_gen(runner, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_iterable)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NestedEventLoops:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnested_async_message)\n",
      "\u001b[0;31mInvalidError\u001b[0m: You can't run Function.map() or Function.for_each() from an async function. Use Function.map.aio()/Function.for_each.aio() instead."
     ]
    }
   ],
   "source": [
    "import modal\n",
    "import time \n",
    "\n",
    "app = modal.App()\n",
    "\n",
    "\n",
    "@app.function(concurrency_limit=10)\n",
    "def func(a, b):\n",
    "    time.sleep(0.3)\n",
    "    return f\"a={a}, b={b}\"\n",
    "\n",
    "with modal.enable_output(), app.run():\n",
    "    print(\"single call\")\n",
    "    print(func.remote(1, 2))\n",
    "\n",
    "    print(\"sequential calls\")\n",
    "    for i in range(20): \n",
    "        print(func.remote(i, i + 1))\n",
    "\n",
    "    print(\"parallel call\")\n",
    "    out = func.starmap([(i, i + 1) for i in range(20)])\n",
    "    for item in out:\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dab197-6f55-46ce-8335-f25094a35eb6",
   "metadata": {},
   "source": [
    "This is particularily painful when we consider that it's the parallel stuff that we are usually super interested in!\n",
    "\n",
    "## Enter joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ae853b-595f-4657-b887-fe4742efd96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def expensive(i):\n",
    "    time.sleep(random.random())\n",
    "    return i\n",
    "\n",
    "p = Parallel(n_jobs=8)(delayed(expensive)(i) for i in range(10))\n",
    "p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a94a634-834d-46d2-abcf-dc1c51e81cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "p = Parallel(n_jobs=8, return_as=\"generator\")(delayed(expensive)(i) for i in range(10))\n",
    "\n",
    "for item in p:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3147f2ea-ccfe-434e-a8aa-700572dfab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "8\n",
      "7\n",
      "2\n",
      "5\n",
      "9\n",
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "p = Parallel(n_jobs=8, return_as=\"generator_unordered\")(delayed(expensive)(i) for i in range(10))\n",
    "\n",
    "for item in p:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7c88c-1998-4a55-9da5-a0c77dff47e0",
   "metadata": {},
   "source": [
    "The whole point here is that joblib figures out the \"parallel stuff\". This is quite a bit of stuff too: \n",
    "\n",
    "- figuring how to spin up workers\n",
    "- make sure workers have dependencies\n",
    "- make sure we serialize to the workers\n",
    "- make sure that we can serialize stuff back\n",
    "- try and be fault tolerant about it\n",
    "\n",
    "But what if we want to use huge cloud resources and many machines? Backends!\n",
    "\n",
    "- [Dask](https://joblib.readthedocs.io/en/latest/auto_examples/parallel/distributed_backend_simple.html)\n",
    "- [Ray](https://docs.ray.io/en/latest/ray-more-libs/joblib.html)\n",
    "\n",
    "But now also Modal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2491752-bb46-418c-88ea-bf604dd64d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to register the modal backend\n",
    "import joblib_modal\n",
    "from joblib import parallel_config, Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "470cd0bc-b8b1-472b-8b97-9c9c923894e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainThread] 2025-01-17T12:49:05+0100 Warning: the results of a call to Function.starmap was not consumed, so the call will never be executed. Consider a for-loop like `for x in Function.starmap(...)` or unpacking the generator using `list(...)`\n"
     ]
    }
   ],
   "source": [
    "with parallel_config(backend=\"modal\", name=\"my-test-job\"):\n",
    "    out = Parallel(n_jobs=-1)(delayed(expensive)(i) for i in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1240df18-3876-4e24-8c7b-201d4b8faddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2841d-ac07-4d23-9322-2a9ff86a4df8",
   "metadata": {},
   "source": [
    "When is this super duper useful? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed0d6e53-3a03-4212-870e-8c09515cfbf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import modal\n",
    "import numpy as np\n",
    "import joblib\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "image = (\n",
    "  modal.Image.debian_slim()\n",
    "  .pip_install(f\"scikit-learn=={sklearn.__version__}\")\n",
    "  .pip_install(f\"numpy=={np.__version__}\")\n",
    "  .pip_install(f\"joblib=={joblib.__version__}\")\n",
    "  .pip_install(f\"scipy=={scipy.__version__}\")\n",
    ")\n",
    "\n",
    "param_grid = {'learning_rate': np.logspace(-3, -1, 100), 'max_depth': [3, 5, 7, 9, 11], \"random_state\": [42, 43, 44]}\n",
    "\n",
    "clf = HistGradientBoostingClassifier()\n",
    "grid_search = RandomizedSearchCV(clf, param_grid, cv=5, n_jobs=-1, n_iter=200)\n",
    "X, y = make_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b05018-040b-4d03-aeef-a9d06bac03d0",
   "metadata": {},
   "source": [
    "This benchmark is silly. But notice the speed/scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f83aeda8-0592-4e43-bcb3-1e2eee538fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.59 s, sys: 5.82 s, total: 8.42 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with parallel_config(\n",
    "    backend=\"modal\",\n",
    "    n_jobs=-1,\n",
    "    name=\"test-joblib\",\n",
    "    image=image,\n",
    "    modal_output=False,\n",
    "):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a0eb3c-7b00-4098-8228-49911243b433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(grid_search.cv_results_).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9012fc88-2cb2-4d94-b293-07ace426897c",
   "metadata": {},
   "source": [
    "The focus is gridsearch for now, but I figured I might try to use it for something else as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a09f04a-8639-40b6-9044-386cc85acb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "texts = pd.read_csv(\"sentences.csv\", nrows=50_000)[\"sentence\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d4fe1ef-f02b-4890-964a-51a3ccfea9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_items(items, batch_size=1000):\n",
    "    batch = []\n",
    "    for item in items:\n",
    "        batch.append(item)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:  # Don't forget the last partial batch\n",
    "        yield batch\n",
    "\n",
    "batches = batch_items(texts, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e618fa9-43f8-48dc-bd07-2cdd121f0e95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import parallel_config, Parallel, delayed\n",
    "import modal\n",
    "import joblib_modal\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "image = (\n",
    "    modal.Image.debian_slim()\n",
    "    .pip_install(\"sentence-transformers\")\n",
    "    .run_commands(\n",
    "        f\"python -c 'from sentence_transformers import SentenceTransformer; tfm = SentenceTransformer(\\\"{EMBEDDING_MODEL}\\\")'\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    return SentenceTransformer(EMBEDDING_MODEL).encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4144220-60ce-4816-b239-e8e7ac757d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainThread] 2025-01-15T16:48:50+0100 Warning: the results of a call to Function.starmap was not consumed, so the call will never be executed. Consider a for-loop like `for x in Function.starmap(...)` or unpacking the generator using `list(...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.09 s, sys: 964 ms, total: 3.06 s\n",
      "Wall time: 41.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batches = batch_items(texts, batch_size=1000)\n",
    "\n",
    "with parallel_config(\n",
    "    backend=\"modal\", name=\"my-emb-job\", image=image, modal_output=False\n",
    "):\n",
    "    out = Parallel(n_jobs=-1)(delayed(embed)(t) for t in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1efef12e-e3da-4696-9e20-7618a750b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 384)\n"
     ]
    }
   ],
   "source": [
    "print(np.concatenate(out).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c48dc-45ec-41ea-a33e-cd63337d226a",
   "metadata": {},
   "source": [
    "In this case though, we might also try to run a single GPU function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee995958-377d-4269-b401-d7fd8a4f0942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.8 s, sys: 1.33 s, total: 4.13 s\n",
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batches = batch_items(texts, batch_size=500)\n",
    "\n",
    "with parallel_config(\n",
    "    backend=\"modal\", name=\"my-emb-job\", image=image, modal_output=False,\n",
    "):\n",
    "    out = Parallel(n_jobs=-1)(delayed(embed)(t) for t in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78e4cb71-4e9b-4db1-a7ec-730f26fe2013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.58 s, sys: 1.54 s, total: 4.11 s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batches = batch_items(texts, batch_size=250)\n",
    "\n",
    "with parallel_config(\n",
    "    backend=\"modal\", name=\"my-emb-job\", image=image, modal_output=False\n",
    "):\n",
    "    out = Parallel(n_jobs=-1)(delayed(embed)(t) for t in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f0f4d5-897c-4b63-8c8f-073f76adcf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "\n",
    "image = (\n",
    "    modal.Image.debian_slim()\n",
    "      .pip_install(f\"sentence-transformers\")\n",
    "      .run_commands(f\"python -c 'from sentence_transformers import SentenceTransformer; tfm = SentenceTransformer(\\\"{EMBEDDING_MODEL}\\\")'\")\n",
    ")\n",
    "\n",
    "app = modal.App(image=image)\n",
    "\n",
    "@app.function(gpu=\"A100\")\n",
    "def func(texts):\n",
    "    return SentenceTransformer(EMBEDDING_MODEL).encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a5377f9-9d10-49ee-a98e-2f09e4388e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 504 ms, sys: 349 ms, total: 852 ms\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with app.run():\n",
    "    out = func.remote(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e3b2fcd-45ca-409c-a4c7-d40f20d6ee83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 384)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c748e8b7-00c6-4890-966e-2322b448a7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 15.8 s, total: 1min 55s\n",
      "Wall time: 3min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00275143,  0.05215697,  0.02795227, ..., -0.00358941,\n",
       "         0.04294354,  0.03718437],\n",
       "       [-0.01325271, -0.03367144, -0.07323536, ...,  0.01747229,\n",
       "         0.02711525,  0.016857  ],\n",
       "       [-0.03211771,  0.04568438,  0.04654278, ...,  0.02424458,\n",
       "         0.01672448, -0.02681892],\n",
       "       ...,\n",
       "       [-0.03979724, -0.10982806,  0.01440763, ..., -0.03324683,\n",
       "        -0.03124548,  0.02747727],\n",
       "       [-0.13533212,  0.00873163,  0.01361843, ..., -0.0047535 ,\n",
       "        -0.00705758, -0.00325262],\n",
       "       [-0.07847907,  0.01026217, -0.03207597, ..., -0.03375116,\n",
       "        -0.08578945,  0.02612685]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SentenceTransformer(EMBEDDING_MODEL).encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6caad8de-95fa-402b-8a81-84f0c72cb16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94bd3ad0-0f79-48e0-8b60-2de2ae3ca61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03088"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_per_s = 0.000772\n",
    "40 * gpu_per_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ea8431d-6555-4566-9cb9-2757ea4f0f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015200000000000002"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This job that 200 workers in total and logs suggest that each worker spent about 2s.\n",
    "\n",
    "cpu_per_s = 0.000038\n",
    "200 * 2 * cpu_per_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea12403-0422-4149-8ba8-4755bc876dd2",
   "metadata": {},
   "source": [
    "Take these numbers with a grain of salt because it is unclear how compute for cold/warm state is calculated. Memory is also not taken into account. But it remains an interesting result. There might be some jobs that are faster on a whole lot of CPUs instead of a GPU.\n",
    "\n",
    "Things not to forget, joblib integration right now:\n",
    "\n",
    "- configure timeout?\n",
    "- configure GPU?\n",
    "- configure datasets?\n",
    "- warm start memory? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c622a-07c0-4589-b850-8ac67d3f3195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
